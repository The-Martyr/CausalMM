# CausalMM: Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality
<p align="center" width="100%">
<a target="_blank"><img src="imgs/pipeline.png" alt="CausalMM" style="width: 35%; min-width: 200px; display: block; margin: auto;"></a>
</p>

The office repo for CausalMM, a plug-and-play method for deciphering attention causality in MLLMs. 
Full paper can be found at: [https://arxiv.org/abs/2410.04780](https://arxiv.org/abs/2410.04780).

<div style='display:flex; gap: 0.25rem; '>
<a href='https://arxiv.org/abs/2410.04780'><img src='https://img.shields.io/badge/Paper-PDF-red'></a>
<a href='LICENCE'><img src='https://img.shields.io/badge/License-Apache 2.0-g.svg'></a>
</div>

## Update
* [2024-10] Detailed instructions coming soon.
* [2024-10] Key code for editing attention released.

## Structural Causal Model
<p align="center" width="100%">
<a target="_blank"><img src="imgs/causal_graph.png" alt="SCM" style="width: 80%; min-width: 200px; display: block; margin: auto;"></a>
</p>

## Citation
Welcome to star our repo and cite our work:
```
@misc{zhou2024mitigatingmodalitypriorinducedhallucinations,
      title={Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality}, 
      author={Guanyu Zhou and Yibo Yan and Xin Zou and Kun Wang and Aiwei Liu and Xuming Hu},
      year={2024},
      eprint={2410.04780},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.04780}, 
}
```

## Acknowledgement
* [VCD](https://github.com/DAMO-NLP-SG/VCD)
* [OPEAR](https://github.com/shikiw/OPERA?tab=readme-ov-file)
* [LLaVA](https://github.com/haotian-liu/LLaVA)
* [Qwen2-VL](https://github.com/QwenLM/Qwen2-VL)

